{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba8291a",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662509a",
   "metadata": {},
   "source": [
    "## Outline\n",
    "> [***Regression Algorithms:***](#Regression)\n",
    "> - [Linear Regression](#LinearRegression)\n",
    ">\n",
    "> [***Classification Algortithms:***](#Classification)\n",
    "> - [Logistic Regression](#LogReg)\n",
    "> - [Decision Trees](#Decision_Trees)\n",
    "> - [XGBoost](#XGB)\n",
    ">\n",
    "> [***Clustring Algorithms:***](#Clustring)\n",
    "> - [K-means algorithm](#Kmean)\n",
    "> - [Anomaly Detection](#AnomalyDe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acb5e9",
   "metadata": {},
   "source": [
    "<a name='Regression'></a>\n",
    "## Regression Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5895eb",
   "metadata": {},
   "source": [
    "<a name=\"LinearRegression\"></a>\n",
    "### Linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13152f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "        y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "        w, b (scalar): Parameters of the model\n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0] \n",
    "    \n",
    "    # You need to return this variable correctly\n",
    "    total_cost = 0\n",
    "    \n",
    "    ### START CODE HERE ###  \n",
    "    for i in range(m):\n",
    "        f_i = w * x[i] + b\n",
    "        total_cost += (f_i - y[i]) ** 2\n",
    "        \n",
    "    total_cost /= (2*m)\n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81a5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
    "      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "      w, b (scalar): Parameters of the model  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    for i in range(m):\n",
    "        f_i = w * x[i] + b\n",
    "        error = f_i - y[i]\n",
    "        \n",
    "        dj_dw += error * x[i]\n",
    "        dj_db += error\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    ### END CODE HERE ### \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27026a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x :    (ndarray): Shape (m,)\n",
    "      y :    (ndarray): Shape (m,)\n",
    "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(x)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration — primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db = gradient_function(x, y, w, b )  \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(x, y, w, b)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            w_history.append(w)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1])}   \")\n",
    "        \n",
    "    return w, b, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198c2b4",
   "metadata": {},
   "source": [
    "<a name='Classification'></a>\n",
    "\n",
    "---\n",
    "\n",
    "## Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95b967",
   "metadata": {},
   "source": [
    "<a name='LogReg'></a>\n",
    "### Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4baa045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    ### START CODE HERE ### \n",
    "    return (1 / (1 + np.exp(-z)))\n",
    "    ### END SOLUTION ###  \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02b5b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_= 0):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: unused placeholder\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = x.shape\n",
    "    error = 0\n",
    "    reg_cost = 0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(w, x[i]) + b\n",
    "        f_i = get_sigmoid(z_i)\n",
    "        \n",
    "        error += -y[i] * np.log(f_i) -(1-y[i]) * np.log(1 - f_i) \n",
    "    error /= m \n",
    "    \n",
    "    # Regulization part\n",
    "    for j in range(n):\n",
    "        reg_cost += w[j] ** 2\n",
    "    reg_cost *= (lambda_ / (2*m))\n",
    "    \n",
    "    Total_cost = error + reg_cost\n",
    "    return Total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7662f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=0): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: unused placeholder.\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    dj_dw = np.zeros(x.shape[1])\n",
    "    dj_db = 0\n",
    "    m, n = x.shape\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_i = get_sigmoid(np.dot(w, x[i]) + b)\n",
    "        error = f_i - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * x[i, j]\n",
    "        dj_db += error\n",
    "    dj_db /= m\n",
    "    dj_dw /= m\n",
    "    \n",
    "    # Regulization part\n",
    "    for j in range(n):\n",
    "        dj_dw[j] += (lambda_ / m) * w[j]\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ac21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(numOfIterations):\n",
    "        dj_db, dj_dw = compute_gradient(x, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        cost = compute_cost(x, y, w, b)\n",
    "        J_history.append(cost)\n",
    "        if i% math.ceil(numOfIterations / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "    return b, w, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b5c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        z = np.dot(X[i], w) + b\n",
    "        f_i = sigmoid(z)\n",
    "\n",
    "        # Apply the threshold\n",
    "        p[i] = round(f_i)\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e33fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    score = 0\n",
    "    for i in range(m):\n",
    "        predicted_value_i = get_sigmoid(np.dot(final_w, X_train[i]) + final_b)\n",
    "        if round(predicted_value_i) == y[i]:\n",
    "            score += 1\n",
    "    return f'Predicting score is {(score / m) * 100}%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54176a5",
   "metadata": {},
   "source": [
    "<a name='Decision_Trees'></a>\n",
    "### Descision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe7e4e",
   "metadata": {},
   "source": [
    "- Recall that the steps for building a decision tree are as follows:\n",
    "    - Start with all examples at the root node\n",
    "    - Calculate information gain for splitting on all possible features, and pick the one with the highest information gain\n",
    "    - Split dataset according to the selected feature, and create left and right branches of the tree\n",
    "    - Keep repeating splitting process until stopping criteria is met\n",
    "  \n",
    "  \n",
    "- In this lab, you'll implement the following functions, which will let you split a node into left and right branches using the feature with the highest information gain\n",
    "    - Calculate the entropy at a node \n",
    "    - Split the dataset at a node into left and right branches based on a given feature\n",
    "    - Calculate the information gain from splitting on a given feature\n",
    "    - Choose the feature that maximizes information gain\n",
    "    \n",
    "- We'll then use the helper functions you've implemented to build a decision tree by repeating the splitting process until the stopping criteria is met \n",
    "    - For this lab, the stopping criteria we've chosen is setting a maximum depth of 2\n",
    "    \n",
    "    \n",
    "**Calculate entropy**\n",
    "\n",
    "First, you'll write a helper function called `compute_entropy` that computes the entropy (measure of impurity) at a node. \n",
    "- The function takes in a numpy array (`y`) that indicates whether the examples in that node are edible (`1`) or poisonous(`0`) \n",
    "\n",
    "Complete the `compute_entropy()` function below to:\n",
    "* Compute $p_1$, which is the fraction of examples that are edible (i.e. have value = `1` in `y`)\n",
    "* The entropy is then calculated as \n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "* Note \n",
    "    * The log is calculated with base $2$\n",
    "    * For implementation purposes, $0\\text{log}_2(0) = 0$. That is, if `p_1 = 0` or `p_1 = 1`, set the entropy to `0`\n",
    "    * Make sure to check that the data at a node is not empty (i.e. `len(y) != 0`). Return `0` if it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for \n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           edible (`1`) or poisonous (`0`)\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "        \n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly\n",
    "    entropy = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    if len(y) != 0:\n",
    "        p       = y.mean()\n",
    "        p_      = 1 - p\n",
    "        \n",
    "        if p == 0:\n",
    "            entropy = -p_ * np.log2(p_)\n",
    "        elif p == 1:\n",
    "            entropy = -p  * np.log2(p)\n",
    "        else:\n",
    "            entropy = - (p * np.log2(p) + p_ * np.log2(p_))\n",
    "            \n",
    "        return entropy\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        return 0\n",
    "    ### END CODE HERE ###        \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "782631e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for node in node_indices:\n",
    "        if X[node, feature] == 1:\n",
    "            left_indices.append(node)\n",
    "        else:\n",
    "            right_indices.append(node)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476ba09",
   "metadata": {},
   "source": [
    "**Calculate information gain**\n",
    "\n",
    "Next, you'll write a function called `information_gain` that takes in the training data, the indices at a node and a feature to split on and returns the information gain from the split.\n",
    "\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$\n",
    "\n",
    "where \n",
    "- $H(p_1^\\text{node})$ is entropy at the node \n",
    "- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ are the entropies at the left and the right branches resulting from the split\n",
    "- $w^{\\text{left}}$ and $w^{\\text{right}}$ are the proportion of examples at the left and right branch, respectively\n",
    "\n",
    "Note:\n",
    "- You can use the `compute_entropy()` function that you implemented above to calculate the entropy\n",
    "- We've provided some starter code that uses the `split_dataset()` function you implemented above to split the dataset \n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "   \n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    information_gain = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    H_root  = compute_entropy(y_node)\n",
    "    H_left  = compute_entropy(y_left)\n",
    "    H_right = compute_entropy(y_right)\n",
    "    \n",
    "    w_left  = len(y_left)  / len(y_node)\n",
    "    w_right = len(y_right) / len(y_node)\n",
    "    \n",
    "    information_gain = H_root - (w_left * H_left + w_right * H_right)\n",
    "    ### END CODE HERE ###  \n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27505dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    maximum_IG   = 0 \n",
    "    ### START CODE HERE ###\n",
    "    for feture in range(num_features):\n",
    "        IG = compute_information_gain(X, y, node_indices, feture)\n",
    "        if IG > maximum_IG:\n",
    "            best_feature = feture\n",
    "            maximum_IG   = IG\n",
    "    ### END CODE HERE ##    \n",
    "   \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6041247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe76786",
   "metadata": {},
   "source": [
    "<a name='XGB'></a>\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4389b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(x_training, y_training)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d77cdc",
   "metadata": {},
   "source": [
    "---\n",
    "<a name='Clustring'></a>\n",
    "## Clustring Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21ae0f",
   "metadata": {},
   "source": [
    "<a name='Kmean'></a>\n",
    "### K-means algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7432501",
   "metadata": {},
   "source": [
    "**Finding closest centroids**\n",
    "\n",
    "In the “cluster assignment” phase of the K-means algorithm, the\n",
    "algorithm assigns every training example $x^{(i)}$ to its closest\n",
    "centroid, given the current positions of centroids. \n",
    "\n",
    "* This function takes the data matrix `X` and the locations of all\n",
    "centroids inside `centroids` \n",
    "* It should output a one-dimensional array `idx` (which has the same number of elements as `X`) that holds the index  of the closest centroid (a value in $\\{1,...,K\\}$, where $K$ is total number of centroids) to every training example .\n",
    "* Specifically, for every example $x^{(i)}$ we set\n",
    "$$c^{(i)} := j \\quad \\mathrm{that \\; minimizes} \\quad ||x^{(i)} - \\mu_j||^2,$$\n",
    "where \n",
    " * $c^{(i)}$ is the index of the centroid that is closest to $x^{(i)}$ (corresponds to `idx[i]` in the starter code), and \n",
    " * $\\mu_j$ is the position (value) of the $j$’th centroid. (stored in `centroids` in the starter code)\n",
    " \n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f535ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_centroids(X, centroids):\n",
    "    \"\"\"\n",
    "    Computes the centroid memberships for every example\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Input values      \n",
    "        centroids (ndarray): k centroids\n",
    "    \n",
    "    Returns:\n",
    "        idx (array_like): (m,) closest centroids\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Set K\n",
    "    K = centroids.shape[0]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    n = X.shape[0]\n",
    "    distances = np.zeros(centroids.shape[0])\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            distances[k] = np.sum(np.subtract(centroids[k], X[i]) ** 2)\n",
    "\n",
    "        idx[i] = np.argmin(distances)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0cb73b",
   "metadata": {},
   "source": [
    "**Computing centroid means**\n",
    "\n",
    "Given assignments of every point to a centroid, the second phase of the\n",
    "algorithm recomputes, for each centroid, the mean of the points that\n",
    "were assigned to it.\n",
    "\n",
    "* Specifically, for every centroid $\\mu_k$ we set\n",
    "$$\\mu_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x^{(i)}$$ \n",
    "\n",
    "    where \n",
    "    * $C_k$ is the set of examples that are assigned to centroid $k$\n",
    "    * $|C_k|$ is the number of examples in the set $C_k$\n",
    "\n",
    "\n",
    "* Concretely, if two examples say $x^{(3)}$ and $x^{(5)}$ are assigned to centroid $k=2$,\n",
    "then you should update $\\mu_2 = \\frac{1}{2}(x^{(3)}+x^{(5)})$.\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a30282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(X, idx, K):\n",
    "    \"\"\"\n",
    "    Returns the new centroids by computing the means of the \n",
    "    data points assigned to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):   (m, n) Data points\n",
    "        idx (ndarray): (m,) Array containing index of closest centroid for each \n",
    "                       example in X. Concretely, idx[i] contains the index of \n",
    "                       the centroid closest to example i\n",
    "        K (int):       number of centroids\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): (K, n) New centroids computed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Useful variables\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    centroids = np.zeros((K, n))\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for k in range(K):\n",
    "        nodes = X[np.where(idx==k)]\n",
    "        centroids[k] = nodes.mean(0)\n",
    "    ### END CODE HERE ## \n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kMeans(X, initial_centroids, max_iters=10, plot_progress=False):\n",
    "    \"\"\"\n",
    "    Runs the K-Means algorithm on data matrix X, where each row of X\n",
    "    is a single example\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize values\n",
    "    m, n = X.shape\n",
    "    K = initial_centroids.shape[0]\n",
    "    centroids = initial_centroids\n",
    "    previous_centroids = centroids    \n",
    "    idx = np.zeros(m)\n",
    "    \n",
    "    # Run K-Means\n",
    "    for i in range(max_iters):\n",
    "        \n",
    "        #Output progress\n",
    "        print(\"K-Means iteration %d/%d\" % (i, max_iters-1))\n",
    "        \n",
    "        # For each example in X, assign it to the closest centroid\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "        \n",
    "        # Optionally plot progress\n",
    "        if plot_progress:\n",
    "            plot_progress_kMeans(X, centroids, previous_centroids, idx, K, i)\n",
    "            previous_centroids = centroids\n",
    "            \n",
    "        # Given the memberships, compute new centroids\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "    plt.show() \n",
    "    return centroids, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367e203",
   "metadata": {},
   "source": [
    "**Random initialization**\n",
    "\n",
    "The initial assignments of centroids for the example dataset was designed so that you will see the same figure as in Figure 1. In practice, a good strategy for initializing the centroids is to select random examples from the\n",
    "training set.\n",
    "\n",
    "In this part of the exercise, you should understand how the function `kMeans_init_centroids` is implemented.\n",
    "* The code first randomly shuffles the indices of the examples (using `np.random.permutation()`). \n",
    "* Then, it selects the first $K$ examples based on the random permutation of the indices. \n",
    "    * This allows the examples to be selected at random without the risk of selecting the same example twice.\n",
    "\n",
    "**Note**: You do not need to make implement anything for this part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2f2de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans_init_centroids(X, K):\n",
    "    \"\"\"\n",
    "    This function initializes K centroids that are to be \n",
    "    used in K-Means on the dataset X\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Data points \n",
    "        K (int):     number of centroids/clusters\n",
    "    \n",
    "    Returns:\n",
    "        centroids (ndarray): Initialized centroids\n",
    "    \"\"\"\n",
    "    \n",
    "    # Randomly reorder the indices of examples\n",
    "    randidx = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    # Take the first K examples as centroids\n",
    "    centroids = X[randidx[:K]]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3669acc",
   "metadata": {},
   "source": [
    "<a name='AnomalyDe'></a>\n",
    "### Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120339e9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Estimating parameters for the gaussian \"normal\" distribution**\n",
    "\n",
    "You can estimate the parameters, ($\\mu_i$, $\\sigma_i^2$), of the $i$-th\n",
    "feature by using the following equations. To estimate the mean, you will\n",
    "use:\n",
    "\n",
    "$$\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)}$$\n",
    "\n",
    "and for the variance you will use:\n",
    "$$\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2$$\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gaussian(X): \n",
    "    \"\"\"\n",
    "    Calculates mean and variance of all features \n",
    "    in the dataset\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Data matrix\n",
    "    \n",
    "    Returns:\n",
    "        mu (ndarray): (n,) Mean of all features\n",
    "        var (ndarray): (n,) Variance of all features\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    mu  = X.mean(0)\n",
    "    var = X.var(0)\n",
    "    ### END CODE HERE ### \n",
    "        \n",
    "    return mu, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbe21f",
   "metadata": {},
   "source": [
    "**Selecting the threshold $\\epsilon$**\n",
    "\n",
    "Now that you have estimated the Gaussian parameters, you can investigate which examples have a very high probability given this distribution and which examples have a very low probability.  \n",
    "\n",
    "* The low probability examples are more likely to be the anomalies in our dataset. \n",
    "* One way to determine which examples are anomalies is to select a threshold based on a cross validation set. \n",
    "\n",
    "In this section, you will complete the code in `select_threshold` to select the threshold $\\varepsilon$ using the $F_1$ score on a cross validation set.\n",
    "\n",
    "* For this, we will use a cross validation set\n",
    "$\\{(x_{\\rm cv}^{(1)}, y_{\\rm cv}^{(1)}),\\ldots, (x_{\\rm cv}^{(m_{\\rm cv})}, y_{\\rm cv}^{(m_{\\rm cv})})\\}$, where the label $y=1$ corresponds to an anomalous example, and $y=0$ corresponds to a normal example. \n",
    "* For each cross validation example, we will compute $p(x_{\\rm cv}^{(i)})$. The vector of all of these probabilities $p(x_{\\rm cv}^{(1)}), \\ldots, p(x_{\\rm cv}^{(m_{\\rm cv)}})$ is passed to `select_threshold` in the vector `p_val`. \n",
    "* The corresponding labels $y_{\\rm cv}^{(1)}, \\ldots, y_{\\rm cv}^{(m_{\\rm cv)}}$ is passed to the same function in the vector `y_val`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00b650",
   "metadata": {},
   "source": [
    "* In the provided code `select_threshold`, there is already a loop that will try many different values of $\\varepsilon$ and select the best $\\varepsilon$ based on the $F_1$ score. \n",
    "\n",
    "* You need implement code to calculate the F1 score from choosing `epsilon` as the threshold and place the value in `F1`. \n",
    "\n",
    "  * Recall that if an example $x$ has a low probability $p(x) < \\varepsilon$, then it is classified as an anomaly. \n",
    "        \n",
    "  * Then, you can compute precision and recall by: \n",
    "   $$\\begin{aligned}\n",
    "   prec&=&\\frac{tp}{tp+fp}\\\\\n",
    "   rec&=&\\frac{tp}{tp+fn},\n",
    "   \\end{aligned}$$ where\n",
    "    * $tp$ is the number of true positives: the ground truth label says it’s an anomaly and our algorithm correctly classified it as an anomaly.\n",
    "    * $fp$ is the number of false positives: the ground truth label says it’s not an anomaly, but our algorithm incorrectly classified it as an anomaly.\n",
    "    * $fn$ is the number of false negatives: the ground truth label says it’s an anomaly, but our algorithm incorrectly classified it as not being anomalous.\n",
    "\n",
    "  * The $F_1$ score is computed using precision ($prec$) and recall ($rec$) as follows:\n",
    "    $$F_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}$$ \n",
    "\n",
    "**Implementation Note:** \n",
    "In order to compute $tp$, $fp$ and $fn$, you may be able to use a vectorized implementation rather than loop over all the examples.\n",
    "\n",
    "\n",
    "If you get stuck, you can check out the hints presented after the cell below to help you with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53597be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_threshold(y_val, p_val): \n",
    "    \"\"\"\n",
    "    Finds the best threshold to use for selecting outliers \n",
    "    based on the results from a validation set (p_val) \n",
    "    and the ground truth (y_val)\n",
    "    \n",
    "    Args:\n",
    "        y_val (ndarray): Ground truth on validation set\n",
    "        p_val (ndarray): Results on validation set\n",
    "        \n",
    "    Returns:\n",
    "        epsilon (float): Threshold chosen \n",
    "        F1 (float):      F1 score by choosing epsilon as threshold\n",
    "    \"\"\" \n",
    "\n",
    "    best_epsilon = 0\n",
    "    best_F1 = 0\n",
    "    F1 = 0\n",
    "    \n",
    "    step_size = (max(p_val) - min(p_val)) / 1000\n",
    "    \n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "    \n",
    "        ### START CODE HERE ### \n",
    "        predictions = (p_val < epsilon)\n",
    "        \n",
    "        tp = np.logical_and((predictions == 1), (y_val == 1)).sum()\n",
    "        fp = np.logical_and((predictions == 1), (y_val == 0)).sum()\n",
    "        fn = np.logical_and((predictions == 0), (y_val == 1)).sum()\n",
    "        \n",
    "        recall    = tp / (tp + fp)\n",
    "        precision = tp / (tp + fn)\n",
    "        \n",
    "        F1 = 2 * ((recall * precision) / (recall + precision))\n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        if F1 > best_F1:\n",
    "            best_F1 = F1\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "    return best_epsilon, best_F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
